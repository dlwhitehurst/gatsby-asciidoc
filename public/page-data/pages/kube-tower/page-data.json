{"componentChunkName":"component---src-templates-article-js","path":"/pages/kube-tower/","result":{"data":{"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#_introduction\">1. Introduction</a></li>\n<li><a href=\"#_preliminaries\">2. Preliminaries</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_goal\">2.1. Goal</a></li>\n<li><a href=\"#_platform\">2.2. Platform</a></li>\n<li><a href=\"#_dependencies\">2.3. Dependencies</a></li>\n</ul>\n</li>\n<li><a href=\"#_installation\">3. Installation</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_operating_system\">3.1. Operating System</a></li>\n<li><a href=\"#_update_and_upgrade\">3.2. Update and Upgrade</a></li>\n<li><a href=\"#_provisioning\">3.3. Provisioning</a></li>\n<li><a href=\"#_testing_and_health\">3.4. Testing and Health</a></li>\n</ul>\n</li>\n<li><a href=\"#_retrospective_and_notes\">4. Retrospective and Notes</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_overall_process\">4.1. Overall Process</a></li>\n<li><a href=\"#_education\">4.2. Education</a></li>\n<li><a href=\"#_automation\">4.3. Automation</a></li>\n</ul>\n</li>\n<li><a href=\"#_acronyms\">Acronyms</a></li>\n<li><a href=\"#_glossary\">Glossary</a></li>\n<li><a href=\"#_license\">Appendix A: License</a></li>\n</ul>\n</div>\n<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p><a href=\"https://site.fortytwobinary.com/\">Site Homepage</a></p>\n</div>\n<div class=\"paragraph\">\n<p><a href=\"https://apache.fortytwobinary.com/lab//welcome.html\">Lab Homepage</a></p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"images/vy.png\" alt=\"Company Logo\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Welcome to the FortyTwoBinary service documentation for our Kubernetes Tower, \"Frankie\" or\nor short for Frankenstein. The tower consists of two Raspberry Pi 3 Model B machines and 3 Raspberry Pi 4 Model B machines atop\na G-Technology 500GB <a href=\"#X1002\">NAS</a>. This document is subject to great change and will be maintained in a software repository <a href=\"https://github.com/fortytwobinary/labdocs\">here</a>.</p>\n</div>\n<div class=\"admonitionblock warning\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Warning</div>\n</td>\n<td class=\"content\">\nThis installation reference was drafted and is maintained as a DevOps reference to the work that occurs on the FortyTwoBinary network and infrastructure. The\npowerful command-line instructions found here, may or may not be accurate\nor timely. This warning is given to let the DevOp be aware that commands\nmay not be idempotent and before root-level (sudo) actions are taken, to\nthink twice before hitting the Enter key.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sidebarblock\">\n<div class=\"content\">\n<div class=\"title\">This document</div>\n<div class=\"paragraph\">\n<p>This document was written using asciidoc markup and the HTML you see is\ngenerated by a tool driven using Python called asciidoctor. This\ninstallation reference is a powerful document that can be of great value in the event of data loss, machine crashes, backups, maintenance, installations, and troubleshooting.</p>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"images/frankie.jpg\" alt=\"Frankie\"></span></p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_introduction\">1. Introduction</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>This document will describe the creation of Frankie, my new Raspberry Pi\nKubernetes tower.</p>\n</div>\n<div class=\"paragraph\">\n<p>The tower began with 2 Raspberry Pi 3 Model B machines I had for mining alt-coins or CPU-mined\ncryptocurrency. The 3 Raspberry Pi 4 Model B machines were recently purchased from Labist. They\ncame with heavy metal heatsinks and fans, but the tower kit I later purchased\ncame with tiny quiet fans for each Pi.</p>\n</div>\n<div class=\"paragraph\">\n<p>The G-Technology drive beneath the tower was an external backup disk for my\nApple macbook pro at the time prior to 2008. It&#8217;s 500GB and hosts an ext4\nfilesystem now for the 5 machines to share as an <a href=\"#X1003\">NFS</a> mount.</p>\n</div>\n<div class=\"sidebarblock\">\n<div class=\"content\">\n<div class=\"title\">Disclaimer</div>\n<div class=\"paragraph\">\n<p>Kubernetes has emerged as powerful way to serve web content and move\ndata over the internet at grand scale. This is very expensive even for\nwell-paid consultants to learn on their own. Small computers and virtual\nmachines are inexpensive and convenient to own. My mission here is to\ndescribe the creation of my machine tower and Kubernetes cluster.</p>\n</div>\n<div class=\"paragraph\">\n<p>The instructions described here worked for me however, each step was\nnot without issue because my steps were taken from various online tutorials\nand personal blog postings. Also, I would recommend a written plan before\nany machine configuration. Be sure to perform any step on all machines if\nyou decide not to finish the project in one session. My software versions\nfor Kubernetes are locked, but they also differ by patch versions.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_preliminaries\">2. Preliminaries</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_goal\">2.1. Goal</h3>\n<div class=\"paragraph\">\n<p>The goal here is to install and maintain a Kubernetes (K8s) cluster on a\ntower consisting of a control-plane (master) and 4 worker nodes.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_platform\">2.2. Platform</h3>\n<div class=\"paragraph\">\n<p>As stated in the Goal section above, the platform for installation and\nhosting consists of 5 newer Raspberry Pi machines with Ubuntu 20.04 LTS aarm64\nimages (operating system).</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_dependencies\">2.3. Dependencies</h3>\n<div class=\"paragraph\">\n<p>Dependencies can be machines, wires, hardware, software packages, configurations\nor even tools that need to be available during the process of installation.\nIn this case, the hardware was our foundation. This process requires 5 Raspberry\nPi machines, power, and ethernet cabling to a local network. We also required 5 -\n16GB microSD cards for flashing the operating system.</p>\n</div>\n<div class=\"paragraph\">\n<p>Download Ubuntu 20.04 LTS for aarm64 architecture at the following link: <a href=\"https://ubuntu.com/download/raspberry-pi\">https://ubuntu.com/download/raspberry-pi</a>. Please note that you must select the 64bit choice here.</p>\n</div>\n<div class=\"paragraph\">\n<p>Also, download Balena Etcher for your workstation. <a href=\"https://www.balena.io/etcher/\">https://www.balena.io/etcher/</a>. This application will be used to\nflash or write the boot images (Ubuntu) to your 5 new microSD cards.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_installation\">3. Installation</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The creation of a Kubernetes cluster required the installation of a lighter, Linux server operating system. The new machines came with Raspbian desktop,\na Debian-based distribution for ARM64 processors. We could have stripped or\nuninstalled the desktop but I found an online setup for Kubernetes on Raspberry\nPi by a guy that works for RedHat here in Raleigh NC. He recommended Ubuntu\n20.04 LTS. First, let&#8217;s flash 5 microSD cards with the downloaded Ubuntu image.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_operating_system\">3.1. Operating System</h3>\n<div class=\"paragraph\">\n<p>Place a new or recently erased microSD card in your workstation (working machine).\nOpen BalenaEtcher (was Etcher) and click \"Flash from file\". Select the Ubuntu image in your folder containing the image downloaded earlier.</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"images/etcher.png\" alt=\"etcher\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Click select target and choose the 16GB storage media, then click Flash. Go get a glass of tea. When the flash is complete, repeat the process for the remaining 4\nmicroSD cards. When they are complete, install each card in the machines.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_update_and_upgrade\">3.2. Update and Upgrade</h3>\n<div class=\"paragraph\">\n<p>Let&#8217;s start our machines, update the OS, name our hosts, and shutdown. I&#8217;m going\nto describe this process for one machine. You can do these simultaneously or using\nPutty with multiple sessions if you prefer.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_initial_logon\">3.2.1. Initial Logon</h4>\n<div class=\"paragraph\">\n<p>When you start the new OS, sshd is built in and either you already know the IP or\nyou&#8217;ll need to use your router to determine the local IP. For most people, the IP\nis probably assigned (dynamic host control protocol (DHCP)) by your router. Please\nnote that if you start all of the machines at the same time and you don&#8217;t already\nhave fixed or static (known) IPs, you will not be able to identify machines. Each\nOS is assigned a hostname \"ubuntu\" and unless you know the IPs ahead of time, you\nwill see a list of attached devices on your router, all called \"ubuntu\".</p>\n</div>\n<div class=\"paragraph\">\n<p>When the machine comes online, it&#8217;s hostname is \"ubuntu\" and a sudo user \"ubuntu\"\nhas already been created. Connect to the machine using ssh like so:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$  ssh ubuntu@192.168.1.17</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Enter \"ubuntu\" as the password. The system will immediately prompt for a new\npassword. When the new password is accepted, the ssh session is terminated by the\nsystem. I&#8217;m not sure why, but I assume it&#8217;s a security thing.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_package_updates\">3.2.2. Package Updates</h4>\n<div class=\"paragraph\">\n<p>Log back into the machine and we&#8217;ll now update and upgrade the OS.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo apt-get update &amp;&amp; sudo apt-get upgrade</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Go get another glass of tea. When the upgrade is complete, we&#8217;ll change the\nhostname and shutdown.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_hostnames\">3.2.3. Hostnames</h4>\n<div class=\"paragraph\">\n<p>The new tower consists of a Kubernetes master and 4 workers. We&#8217;ll name master,\n\"kube-master\" and each worker <code>kube-worker-n</code> e.g. <code>kube-worker-3</code>. Open an ssh\nwith the machine you will call <code>kube-master</code>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo vi /etc/hostname</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>There should be one line with <code>ubuntu</code> so replace it with the new hostname. In\nthe past we had to change <code>/etc/hosts</code> too this distribution of Linux does not\nrequire editing <code>/etc/hosts</code>. Change each hostname (all machines) and then reboot.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo shutdown -r now</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>When each machine comes back, it&#8217;s hostname will have changed. Update your Putty\nsessions if you are using saved sessions.</p>\n</div>\n<div class=\"sidebarblock\">\n<div class=\"content\">\n<div class=\"title\">Note</div>\n<div class=\"paragraph\">\n<p>My machines already had static IPs that were reserved when they first came online.\nThese MAC addresses are known to the router and this made identification of the\nmachines easy. Also, I could do this work with all machines running on the new OS.\nI don&#8217;t think static local IPs are required for your eth0 interfaces since the\nnodes all have unique hostnames however, I have 2  <a href=\"#X999\">DNS</a> servers on the network and\nthese machines are managed as \"trusted servers\" with forward and reverse lookup\nentries.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_provisioning\">3.3. Provisioning</h3>\n<div class=\"paragraph\">\n<p>Now we are ready to configure and provision the machines with Kubernetes. During\nthe process of installing Kubernetes, we&#8217;ll also bring each of the cluster nodes\ninto operation. Starting with 5 quiet machines, let&#8217;s start kube-master, or our\ncontrol-plane and set that one up first. The host kube-master will act as a\ncontrol-plane and delegate to the 4 worker nodes.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_all_nodes\">3.3.1. All Nodes</h4>\n<div class=\"paragraph\">\n<p>Before we can install Kubernetes, we need to make a few changes to our machines.\nWe will install the Docker (container platform), change the <a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt\">cgroups</a> driver, and optimize our systems for kernel, memory, and swap features.</p>\n</div>\n<div class=\"paragraph\">\n<p>Let&#8217;s install Docker.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo apt install -y docker.io</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Once the installation is complete, run &#8230;&#8203;</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo docker info</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Notice that the cgroups driver is cgroups and the warnings at the end of the output.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">...\nWARNING: No memory limit support\nWARNING: No swap limit support\nWARNING: No kernel memory limit support\nWARNING: No kernel memory TCP limit support\nWARNING: No oom kill disable support</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This shows us that Docker has no memory, swap, or kernel support. We can fix this\nby changing the cgroups driver to <code>systemd</code>. Systemd is recommended by Kubernetes\nand to ensure better system stability. Let&#8217;s tell our system with specification\nthat cgroups management will now be <code>systemd</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Create or edit this file and contents:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo cat &gt; /etc/docker/daemon.json &lt;&lt;EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\"\n}\nEOF</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now, let&#8217;s influence the kernel, on boot, to use cgroups to limit memory and swap.\nWe will append these options to the end of <code>cmdline.txt</code>. Since we want to add\nspecific text to the end of the file, we&#8217;ll use <code>sed</code> to do this.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\"># Append the cgroups and swap options to the kernel command line\n# Note the space before \"cgroup_enable=cpuset\" and to add a space after the last existing item on the line\n$ sudo sed -i '$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/' /boot/firmware/cmdline.txt</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Reboot the machine and run the <code>sudo docker info</code> again. You will see the cgroups\ndriver is now <code>systemd</code> and the limit warnings are gone.</p>\n</div>\n<div class=\"paragraph\">\n<p>Kubernetes also recommends that iptables and iptables6 be set to see bridged-network traffic.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Activate this configuration with &#8230;&#8203;</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">sudo sysctl --system</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now we need to set up the Kubernetes apt repository and then install the Kubernetes packages. Let&#8217;s first add the Google key and then add the Kubernetes\nrepo to our local list of repositories.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\"># Add the packages.cloud.google.com apt key\n$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n\n# Add the Kubernetes repo to our list of repositories\n$ cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now, we can install the Kubernetes packages to each of our cluster nodes.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo apt update &amp;&amp; sudo apt install -y kubelet kubeadm kubectl</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>We now need to disable any updates from the Kubernetes repo so our installs remain\nconsistent. And, we should handle version updates manually after our cluster is\nin place.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo apt-mark hold kubelet kubeadm kubectl</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Kubernetes is installed!</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_kube_master\">3.3.2. kube-master</h4>\n<div class=\"paragraph\">\n<p>Now that&#8217;s Kubernetes has been installed on all machines, it&#8217;s time to set up and\nconfigure our kube-master host and begin the creation of our Kubernetes cluster.</p>\n</div>\n<div class=\"paragraph\">\n<p>The first thing we are going to do is create a token for the cluster nodes to use\nwhen joining the cluster. We&#8217;ll use kube-master to create a token and then use this token when we initialize the control-plane.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ TOKEN=$(sudo kubeadm token generate)</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Run the following to determine the Kubernetes version.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.4\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-02-18T16:09:38Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/arm64\"}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>My version was 1.20.4 so I add that in the initialization command. Now use this token to initialize the control-plane server (kube-master).</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo kubeadm init --token=${TOKEN} --kubernetes-version=v1.20.5 --pod-network-cidr=10.244.0.0/16</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The CIDR was chosen as an example from other blog postings. More research is needed to understand the CIDR choice. I have no conflicting 10.x.x.x networks on\nour home network.</p>\n</div>\n<div class=\"paragraph\">\n<p>The initialization should be successful and the following output should be seen\nand you should copy this off for safekeeping.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nsudo kubeadm join 192.168.1.12:6443 --ignore-preflight-errors=all --token lm8kpx.fs5six37dossytv6 \\\n    --discovery-token-ca-cert-hash sha256:5d43f3aa2fedfb5e6e4a895a8c160d3a917b1b4776d9cde4001477a53fa88008</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Your control-plane is now running successfully.</p>\n</div>\n<div class=\"paragraph\">\n<p>Make a note of two things. 1) The Kubernetes kubectl connection information has been written to /etc/kubernetes/admin.conf. 2) This kubernetes configuration file can be copied to ~/.kube/config, either for root or a normal user on the master node or to a remote machine. This will allow you to control the cluster with the kubectl command.</p>\n</div>\n<div class=\"paragraph\">\n<p>Use the <code>kubectl</code> utility now to verify that the master node is running.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ kubectl get nodes</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You should see the master node is up and running.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before we join our worker nodes, we need to install a Container Network Interface (CNI) add-on\ncalled Flannel. This add-on provides networking management for our fixed and chosen CIDR. The\nFlannel add-on is easily installed via command-line using a YAML manifest. Apply the following\nmanifest in a file called <code>kube-flannel.yaml</code> like so. The file below is for Kubernetes v1.17+ and our install was 1.20.4.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">curl -sSL https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml | kubectl apply -f -</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Now our worker joins should go without issue. See <a href=\"#_retrospective_and_notes\">Retrospective and Notes</a> below.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_kube_worker_n\">3.3.3. kube-worker-n</h4>\n<div class=\"paragraph\">\n<p>Each worker should now only need to join the cluster. The software and configuration was done\nprior to some specifics only for kube-master. Let&#8217;s send a join command from a worker to master\nand ask to join.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ sudo kubeadm join 192.168.1.12:6443 --token lm8kpx.fs5six37dossytv6 \\\n    --discovery-token-ca-cert-hash sha256:5d43f3aa2fedfb5e6e4a895a8c160d3a917b1b4776d9cde4001477a53fa88008</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Repeat for each worker and then verify that everything is good.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ kubectl get nodes -o wide</code></pre>\n</div>\n</div>\n<div class=\"sidebarblock\">\n<div class=\"content\">\n<div class=\"title\">Note</div>\n<div class=\"paragraph\">\n<p>This did not originally work for me. There was some confusion around the installation of\nFlannel and how it was obtained. I have modified the installation of Flannel above to suit\nwhat I think is correct however, I will do this again in a more automated fashion and I will\ndiscuss my issues with all this in the Retrospective section below.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_testing_and_health\">3.4. Testing and Health</h3>\n<div class=\"paragraph\">\n<p>In a future edit of this document, I&#8217;ll add a really simple deployment to show that our cluster\nis in fact running and viable. For now, I&#8217;ll leave a couple commands I&#8217;ve learned to show the\nhealth and information about our new cluster.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ kubectl cluster-info\nKubernetes control plane is running at https://192.168.1.12:6443\nKubeDNS is running at https://192.168.1.12:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The next command shows critical components. Notice that Flannel and proxy ones are replicated\nfor each node.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">ubuntu@kube-master:~$ ^C\nubuntu@kube-master:~$ kubectl get pod -n kube-system\nNAME                                  READY   STATUS    RESTARTS   AGE\ncoredns-74ff55c5b-9nn7l               1/1     Running   0          42h\ncoredns-74ff55c5b-fcnl2               1/1     Running   0          42h\netcd-kube-master                      1/1     Running   0          42h\nkube-apiserver-kube-master            1/1     Running   0          42h\nkube-controller-manager-kube-master   1/1     Running   5          42h\nkube-flannel-ds-hvlbh                 1/1     Running   4          38h\nkube-flannel-ds-ltgpv                 1/1     Running   0          38h\nkube-flannel-ds-xdzss                 1/1     Running   0          38h\nkube-flannel-ds-zqgbf                 1/1     Running   0          38h\nkube-flannel-ds-zvssl                 1/1     Running   4          38h\nkube-proxy-8fzx5                      1/1     Running   0          39h\nkube-proxy-czrpw                      1/1     Running   4          38h\nkube-proxy-qm7kf                      1/1     Running   4          38h\nkube-proxy-sz5xz                      1/1     Running   0          39h\nkube-proxy-zxnnc                      1/1     Running   0          42h\nkube-scheduler-kube-master            1/1     Running   4          42h\nubuntu@kube-master:~$</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>And, our get nodes again in short format.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">ubuntu@kube-master:~$ kubectl get nodes\nNAME            STATUS   ROLES                  AGE   VERSION\nkube-master     Ready    control-plane,master   42h   v1.20.4\nkube-worker-1   Ready    &lt;none&gt;                 39h   v1.20.5\nkube-worker-2   Ready    &lt;none&gt;                 39h   v1.20.4\nkube-worker-3   Ready    &lt;none&gt;                 39h   v1.20.5\nkube-worker-4   Ready    &lt;none&gt;                 38h   v1.20.5\nubuntu@kube-master:~$</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_retrospective_and_notes\">4. Retrospective and Notes</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>The full administration and use of Vault is yet to be discovered. For now\nwe&#8217;ll provide some cheatsheet commands and a few use cases.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_overall_process\">4.1. Overall Process</h3>\n<div class=\"paragraph\">\n<p>Later &#8230;&#8203; you are welcome lol &#8230;&#8203;</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ vault status</code></pre>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_preparation\">4.1.1. Preparation</h4>\n<div class=\"paragraph\">\n<p>Later &#8230;&#8203;</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_planning\">4.1.2. Planning</h4>\n<div class=\"paragraph\">\n<p>Later &#8230;&#8203;</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_education\">4.2. Education</h3>\n<div class=\"paragraph\">\n<p>Later &#8230;&#8203;</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_automation\">4.3. Automation</h3>\n<div class=\"paragraph\">\n<p>Later &#8230;&#8203;</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_acronyms\">Acronyms</h2>\n<div class=\"sectionbody\">\n<div class=\"hdlist\">\n<table>\n<tr>\n<td class=\"hdlist1\">\n<a id=\"X999\"></a>DNS\n</td>\n<td class=\"hdlist2\">\n<p>Domain Naming System</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<a id=\"X1000\"></a>IP\n</td>\n<td class=\"hdlist2\">\n<p>Internet Protocol</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<a id=\"X1003\"></a>NFS\n</td>\n<td class=\"hdlist2\">\n<p>Network File System</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<a id=\"X1002\"></a>NAS\n</td>\n<td class=\"hdlist2\">\n<p>Network Attached Storage</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nNAT\n</td>\n<td class=\"hdlist2\">\n<p>Network Address Translation</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\n<a id=\"X1001\"></a>SBC\n</td>\n<td class=\"hdlist2\">\n<p>Small board computer</p>\n</td>\n</tr>\n<tr>\n<td class=\"hdlist1\">\nWAP\n</td>\n<td class=\"hdlist2\">\n<p>Wireless Access Point</p>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_glossary\">Glossary</h2>\n<div class=\"sectionbody\">\n<div class=\"dlist glossary\">\n<dl>\n<dt><a id=\"X8\"></a> Block element</dt>\n<dd>\n<p>An AsciiDoc block element is a document entity composed of one or\nmore whole lines of text.</p>\n</dd>\n<dt><a id=\"X34\"></a> Inline element</dt>\n<dd>\n<p>AsciiDoc inline elements occur within block element textual\ncontent, they perform formatting and substitution tasks.</p>\n</dd>\n<dt>Formal element</dt>\n<dd>\n<p>An AsciiDoc block element that has a BlockTitle. Formal elements\nare normally listed in front or back matter, for example lists of\ntables, examples and figures.</p>\n</dd>\n<dt>Verbatim element</dt>\n<dd>\n<p>The word verbatim indicates that white space and line breaks in\nthe source document are to be preserved in the output document.</p>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_license\">Appendix A: License</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>This document is licensed by the Apache License version 2.0. Currently,\nthe content in this document is being kept from the public however, in\nthe event the material contained here is willingly shared with\nothers, the license will remain unchanged and will convey with the\ntransference of the material.</p>\n</div>\n<div class=\"paragraph\">\n<p>Apache License\nVersion 2.0, January 2004\n<a href=\"http://www.apache.org/licenses/\" class=\"bare\">http://www.apache.org/licenses/</a></p>\n</div>\n<div class=\"paragraph\">\n<p>A copy has also been provided with this software repository.</p>\n</div>\n<div class=\"paragraph\">\n<p>Copyright &#169; 2021 David L Whitehurst.</p>\n</div>\n</div>\n</div>","document":{"title":"Kubernetes Tower","subtitle":"","main":"Kubernetes Tower"},"revision":null,"author":{"fullName":"David L. Whitehurst","firstName":"David","lastName":"Whitehurst","middleName":"L.","authorInitials":"DLW","email":""}}},"pageContext":{"id":"8b1fc636-7f71-5bdf-9ba8-f3118e015695"}},"staticQueryHashes":[]}